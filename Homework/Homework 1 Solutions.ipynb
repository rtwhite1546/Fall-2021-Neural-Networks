{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "\n",
    "In this class, students will likely submit many different correct answers on most assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Find a formula for the exact gradient of the sum of squares loss function based on a linear regression model with $d+1$ weights.\n",
    "\n",
    "*Solution.* For linear regression, we have a loss function\n",
    "\n",
    "$$L(w)=\\sum\\limits_{j=1}^n\\left(y_j-f(x_j)\\right)^2=\\sum\\limits_{j=1}^n\\left(y_j-w_0-w_1x_{j1}-\\cdots -w_dx_{jd}\\right)^2$$\n",
    "\n",
    "Taking the derivative with respect to $w_0$, we get\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_0}(w) = 2\\sum\\limits_{j=1}^n\\left(y_j-w_0-w_1x_{j1}-\\cdots -w_dx_{jd}\\right)(-1)$$\n",
    "\n",
    "For each $i=1,...,d$, we have\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i}(w) = 2\\sum\\limits_{j=1}^n\\left(y_j-w_0-w_1x_{j1}-\\cdots -w_dx_{jd}\\right)(-x_{ji})$$\n",
    "\n",
    "Denote $\\hat{f}(x_j)=w_0+\\sum\\limits_{k=1}^d w_kx_{jk}$, then the gradient is\n",
    "\n",
    "$$\\nabla L(w)=\\left(2\\sum\\limits_{j=1}^n\\left(y_j-\\hat{f}(x_j)\\right)(-1),2\\sum\\limits_{j=1}^n\\left(y_j-\\hat{f}(x_j)\\right)(-x_{j1}),...,2\\sum\\limits_{j=1}^n\\left(y_j-\\hat{f}(x_j)\\right)(-x_{jd})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate derivation using matrix notation\n",
    "\n",
    "Let's define some terms:\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}w=\\begin{pmatrix}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then, we have shown the loss function can be written as\n",
    "\n",
    "$$L(w)=\\|Xw-y\\|_2^2$$\n",
    "\n",
    "Let's convert the loss function into a (longer but) simpler form. Transposes can be applied to sums of matrices separately, so\n",
    "\n",
    "$$L(w)=(Xw-y)^T(Xw-y)=((Xw)^T-y^T)(Xw-y)$$\n",
    "\n",
    "Using the distributive property of matrix multiplication,\n",
    "\n",
    "$$L(w)=(Xw)^T Xw-(Xw)^T y-y^T Xw+y^T y$$\n",
    "\n",
    "If we realize $y$ and $Xw$ are both matrices of shape $n\\times 1$, then we should have $(Xw)^T y=y^T Xw$, so the loss function is\n",
    "\n",
    "$$L(w)=(Xw)^T Xw-2(Xw)^T y+y^T y$$\n",
    "\n",
    "Since $(AB)^T=B^TA^T$ for matrices, we can simplify the terms as\n",
    "\n",
    "$$L(w)=w^T X^T Xw-2w^T X^T y+y^T y$$\n",
    "\n",
    "Now, of course, this is a scalar (because, in the end, the loss is just a number--the sum of squared errors), so we can take the derivatives with respect to $w_0$, ..., $w_d$ and put those into a vector as\n",
    "\n",
    "$$\\nabla L(w)=2X^T Xw-2X^T y$$\n",
    "\n",
    "This form is a little easier to use because it's easy to implement in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Write an implementation of linear regression by gradient descent using the exact gradient formula **instead of** a function approximating the gradient (e.g. `computeGradient`).\n",
    "\n",
    "*Solution.* The code below is similar to what we did in class, but the gradient is computed explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LeastSquaresGradient:\n",
    "    # initialize the hyperparameters\n",
    "    def __init__(self, alpha = 0.01, tolerance = 0.01):\n",
    "        self.alpha = alpha\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs):\n",
    "        n = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # add a column of ones to the left side of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # initialize weights to 0\n",
    "        self.w = [0] * (d + 1)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            gradient = X.T @ (X @ self.w - y)\n",
    "            gradientNorm = np.linalg.norm(gradient)\n",
    "            \n",
    "            if gradientNorm < self.tolerance:\n",
    "                print('Gradient descent took', epoch, 'iterations to converge')\n",
    "                print('The norm of the gradient is', gradientNorm)\n",
    "                \n",
    "                # return the approximate critical value w\n",
    "                return self.w\n",
    "            \n",
    "            elif epoch == epochs - 1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                \n",
    "                # return the weights (sometimes it is still pretty good)\n",
    "                return self.w\n",
    "            \n",
    "            self.w -= self.alpha * gradient \n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([n, 1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # compute predictions  \n",
    "        return np.atleast_2d((X @ self.w)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on the U.S. high school graduation rate dataset just to see that it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "\n",
      "Gradient descent took 196 iterations to converge\n",
      "The norm of the gradient is 0.0009910441012740167\n",
      "\n",
      "The mean absolute error on the training set is 3.667144144634513\n",
      "The mean absolute error on the testing set is  4.005061979203928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "\n",
    "# import the data from the csv file to an numpy array\n",
    "data = pd.read_csv('../data/US_State_Data.csv', sep=',').to_numpy()\n",
    "\n",
    "X = np.array(data[:,1:8], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "trainX = scale(trainX)\n",
    "testX = scale(testX)\n",
    "\n",
    "# instantiate a least squares model\n",
    "model = LeastSquaresGradient(alpha = 0.01, tolerance = 0.001)\n",
    "\n",
    "# fit the model to the training data (find the w parameters)\n",
    "print('Fitting the model...\\n')\n",
    "model.fit(trainX, trainY, epochs = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nThe mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the testing set is ', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are equivalent to what we found in class (just a little better, actually), so our implementation seems to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Add elastic-net regularization to your implementation. Hyperparameters $\\lambda_1$ and $\\lambda_2$ should be inputs to the class or the fit function.\n",
    "\n",
    "*Solution.* To add this, we simply need to adjust the gradient to make it the gradient of the following loss function\n",
    "\n",
    "$$L(w)=\\|Xw-y\\|_2^2+\\lambda_1\\|w\\|_1+\\lambda_2\\|w\\|^2$$\n",
    "\n",
    "or, more conveniently,\n",
    "\n",
    "$$L(w)=(Xw-y)^T(Xw-y) + \\lambda_1\\sum\\limits_{i=0}^d |w_i| + \\lambda_2 w^Tw$$\n",
    "\n",
    "We already found the gradient of $(Xw-y)^T(Xw-y)$ to be $2X^T Xw-2X^T y$. The gradient of $\\lambda_2 w^Tw$ is simply $2\\lambda_2w$.\n",
    "\n",
    "For $|w_i|$, the derivative depends on the value of $w_i$, so we have\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_i}|w_i|=\n",
    "\\begin{cases}\n",
    "-1, &\\text{if }w_i<0\\\\\n",
    "\\text{DNE}, &\\text{if }w_i=0\\\\\n",
    "1, &\\text{if }w_i>0\n",
    "\\end{cases}$$\n",
    "\n",
    "**Problem**: the derivative at $w_i=0$ does not actually exist. Practically, we can simply treat the derivative as 0 when this occurs, which will cause the $L^1$ regularization to not act on the parameter.\n",
    "\n",
    "If we initialize the weights to 0, the $L^1$ penalty will not do anything in our first iteration, but as gradient descent causes the weights to become nonzero, the $L^1$ penalty will start having an effect. And, it becomes quite unlikely that the weights will return precisely to 0, so this is not much of a practical concern.\n",
    "\n",
    "Altogether, the gradient of our sum of squared errors loss function with both $L^1$ and $L^2$ regularization will be\n",
    "\n",
    "$$\\nabla L(w)=2X^T Xw-2X^T y+\\lambda_1\\text{sign}(w)+2\\lambda_2w$$\n",
    "\n",
    "Note that the coefficients of 2 are actually somewhat redundant with the multipliers $\\alpha$ in gradient descent weight updates and $\\lambda_2$ for the $L^2$ penalty, so these 2's can be left out of the implementation.\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LeastSquaresGradientRegularized:\n",
    "    # initialize the hyperparameters\n",
    "    def __init__(self, alpha = 0.01, tolerance = 0.01, lam1 = 0, lam2 = 0):\n",
    "        self.alpha = alpha\n",
    "        self.tolerance = tolerance\n",
    "        self.lam1 = lam1\n",
    "        self.lam2 = lam2\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs):\n",
    "        n = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # add a column of ones to the left side of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # initialize weights to 0\n",
    "        self.w = np.array([0] * (d + 1)).astype('float')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            gradient = X.T @ (X @ self.w - y) + self.lam1 * np.sign(self.w) + self.lam2 * self.w\n",
    "            gradientNorm = np.linalg.norm(gradient)\n",
    "            \n",
    "            if gradientNorm < self.tolerance:\n",
    "                print('Gradient descent took', epoch, 'iterations to converge')\n",
    "                \n",
    "                # return the approximate critical value w\n",
    "                return self.w\n",
    "            \n",
    "            elif epoch == epochs - 1:\n",
    "                print(\"Gradient descent failed to converge\")\n",
    "                print('The norm of the gradient is', gradientNorm)\n",
    "                \n",
    "                # return the weights (sometimes it is still pretty good)\n",
    "                return self.w\n",
    "            \n",
    "            self.w -= self.alpha * gradient \n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([n, 1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # compute predictions  \n",
    "        return np.atleast_2d((X @ self.w)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test this regularized implementation in Problem 4 below. Note that, by default, $\\lambda_1=\\lambda_2=0$, meaning the $L^1$ and $L^2$ penalties are turned off, and it has no effect on the gradient, so the model will run identically to the previous implementation.\n",
    "\n",
    "If we set these hyperparameters to nonzero values, they will have an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Load the diabetes dataset from scikit-learn. Tune the hyperparameters of the model to predict disease progression as well as possible. Run the model at least ten different options for the hyperparameters, and document your performance.\n",
    "\n",
    "*Solution.* Let's read in the data and split into train/dev/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of trainX is (265, 10)\n",
      "The dimensions of devX is (88, 10)\n",
      "The dimensions of testX is (88, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# read in the dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# find the data and labels\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "\n",
    "# split the data into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.4)\n",
    "\n",
    "# split the test set into dev and test sets\n",
    "devX, testX, devY, testY = train_test_split(testX, testY, test_size = 0.5)\n",
    "\n",
    "# check the dimensions\n",
    "print('The dimensions of trainX is', trainX.shape)\n",
    "print('The dimensions of devX is', devX.shape)\n",
    "print('The dimensions of testX is', devX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms we have randomly assigned 60% of the data to the training set, 20% to the dev set, and 20% to the test set.\n",
    "\n",
    "We need to train the model with different hyperparameter settings. A good approach is to start with minimal use of hyperparameters until we see what sort of behavior we see, so let's start with $\\lambda_1=\\lambda_2=0$ and tune the learning rate for quick convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Running for alpha = 0.001 ================================\n",
      "Gradient descent failed to converge\n",
      "The norm of the gradient is 0.025587414129761645\n",
      "The mean absolute error on the training set is 44.301048099502694\n",
      "The mean absolute error on the dev set is 42.657354633785374 \n",
      "\n",
      "================================ Running for alpha = 0.002 ================================\n",
      "Gradient descent took 616529 iterations to converge\n",
      "The mean absolute error on the training set is 44.301511933793556\n",
      "The mean absolute error on the dev set is 42.655587893256424 \n",
      "\n",
      "================================ Running for alpha = 0.003 ================================\n",
      "Gradient descent took 411019 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193557534\n",
      "The mean absolute error on the dev set is 42.65558788646962 \n",
      "\n",
      "================================ Running for alpha = 0.004 ================================\n",
      "Gradient descent took 308264 iterations to converge\n",
      "The mean absolute error on the training set is 44.301511937357134\n",
      "The mean absolute error on the dev set is 42.65558787968281 \n",
      "\n",
      "================================ Running for alpha = 0.005 ================================\n",
      "Gradient descent took 246610 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193314113\n",
      "The mean absolute error on the dev set is 42.65558789574155 \n",
      "\n",
      "================================ Running for alpha = 0.006 ================================\n",
      "Gradient descent took 205508 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193372337\n",
      "The mean absolute error on the dev set is 42.6555878935238 \n",
      "\n",
      "================================ Running for alpha = 0.007 ================================\n",
      "Gradient descent took 176150 iterations to converge\n",
      "The mean absolute error on the training set is 44.301511939103854\n",
      "The mean absolute error on the dev set is 42.65558787302953 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in np.linspace(0.001, 0.007, 7):\n",
    "    print('================================ Running for alpha =', alpha, '================================')\n",
    "\n",
    "    # build the linear regression model\n",
    "    model = LeastSquaresGradientRegularized(alpha = alpha)\n",
    "\n",
    "    # fit the linear regression model to the training data\n",
    "    model.fit(trainX, trainY, epochs = 1000000)\n",
    "\n",
    "    # predict the training Y's\n",
    "    predictedY = model.predict(trainX)\n",
    "    print('The mean absolute error on the training set is', mean_absolute_error(predictedY, trainY))\n",
    "\n",
    "    # predict the labels of the training set\n",
    "    predictedY = model.predict(devX)\n",
    "    print('The mean absolute error on the dev set is', mean_absolute_error(predictedY, devY), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the learning rate results in faster convergence, but but if we increase the learning rate, we eventually reach overflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Running for alpha = 0.007 ================================\n",
      "Gradient descent took 176150 iterations to converge\n",
      "The mean absolute error on the training set is 44.301511939103854\n",
      "The mean absolute error on the dev set is 42.65558787302953 \n",
      "\n",
      "================================ Running for alpha = 0.0071 ================================\n",
      "Gradient descent took 173669 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193928204\n",
      "The mean absolute error on the dev set is 42.65558787235085 \n",
      "\n",
      "================================ Running for alpha = 0.0072 ================================\n",
      "Gradient descent took 171257 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151194017995\n",
      "The mean absolute error on the dev set is 42.65558786893072 \n",
      "\n",
      "================================ Running for alpha = 0.0073 ================================\n",
      "Gradient descent took 168911 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151194035812\n",
      "The mean absolute error on the dev set is 42.65558786825205 \n",
      "\n",
      "================================ Running for alpha = 0.0074 ================================\n",
      "Gradient descent took 166628 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193693765\n",
      "The mean absolute error on the dev set is 42.65558788128062 \n",
      "\n",
      "================================ Running for alpha = 0.0075 ================================\n",
      "Gradient descent took 164406 iterations to converge\n",
      "The mean absolute error on the training set is 44.30151193459674\n",
      "The mean absolute error on the dev set is 42.65558789019712 \n",
      "\n",
      "================================ Running for alpha = 0.0076 ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in matmul\n",
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent failed to converge\n",
      "The norm of the gradient is nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7353e5e24885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# predict the training Y's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpredictedY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The mean absolute error on the training set is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# predict the labels of the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \"\"\"\n\u001b[0;32m    178\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 179\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 646\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     (type_err,\n\u001b[1;32m--> 100\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    101\u001b[0m             )\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for alpha in np.linspace(0.007, 0.008, 11):\n",
    "    print('================================ Running for alpha =', alpha, '================================')\n",
    "\n",
    "    # build the linear regression model\n",
    "    model = LeastSquaresGradientRegularized(alpha = alpha)\n",
    "\n",
    "    # fit the linear regression model to the training data\n",
    "    model.fit(trainX, trainY, epochs = 1000000)\n",
    "\n",
    "    # predict the training Y's\n",
    "    predictedY = model.predict(trainX)\n",
    "    print('The mean absolute error on the training set is', mean_absolute_error(predictedY, trainY))\n",
    "\n",
    "    # predict the labels of the training set\n",
    "    predictedY = model.predict(devX)\n",
    "    print('The mean absolute error on the dev set is', mean_absolute_error(predictedY, devY), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(for homework, run more experiments...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Prove $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$ for the sigmoid function $\\sigma$.\n",
    "\n",
    "*Solution*. By definition,\n",
    "\n",
    "$$\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Using the chain rule, the derivaive is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma'(z)&=(-1)\\frac{1}{\\left(1+e^{-z}\\right)^2}e^{-z}(-1)\n",
    "\\\\&=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Adding and subtracting 1 from the numerator of the second fraction,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma'(z)&=\\frac{1}{1+e^{-z}}\\frac{1+e^{-z}-1}{1+e^{-z}}\n",
    "\\\\&=\\frac{1}{1+e^{-z}}\\left(1-\\frac{1}{1+e^{-z}}\\right)\n",
    "\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "As above, let's define:\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}w=\\begin{pmatrix}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where we assume the $y_i$'s are labels in the set $\\{0,1\\}$.\n",
    "\n",
    "Then, we have shown the loss function can be written as\n",
    "\n",
    "$$L(w)=\\|\\sigma(Xw)-y\\|_2^2$$\n",
    "\n",
    "with the $\\sigma$ being applied elementwise to the vector $Xw$.\n",
    "\n",
    "Let's convert the loss function into a (longer but) simpler form. Using the argument from Problem 1 with $Xw$ replaced by $\\sigma(Xw)$, we get\n",
    "\n",
    "$$L(w)=(\\sigma(Xw))^T \\sigma(Xw)-2(\\sigma(Xw))^Ty +y^T y$$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(w) &= 2X^T\\sigma(Xw)\\sigma'(Xw)-2X^T\\sigma'(Xw) y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the result from Problem 5, we can simplify the sigmoid derivatives to find\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(w) &= 2X^T\\sigma(Xw)\\sigma(Xw)\\left(1 - \\sigma(Xw)\\right)-2X^T\\sigma(Xw)\\left(1 - \\sigma(Xw)\\right)y\n",
    "\\\\&= 2X^T\\sigma(Xw)(1 - \\sigma(Xw))\\left(\\sigma(Xw) - y\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class LeastSquaresClassifier:\n",
    "    # initialize the hyperparameters\n",
    "    def __init__(self, alpha = 0.01, tolerance = 0.01):\n",
    "        self.alpha = alpha\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs):\n",
    "        n = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # add a column of ones to the left side of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # initialize weights as uniform random variables between -1 and 1\n",
    "        self.w = 2 * np.random.rand(d + 1) - 1\n",
    "        \n",
    "        # train the model\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # compute sigmoid(Xw), which is reused many times\n",
    "            sigmoidXw = expit(X @ self.w)\n",
    "                        \n",
    "            # compute the gradient\n",
    "            gradient = X.T @ ((sigmoidXw * (1 - sigmoidXw)) * (sigmoidXw - y))\n",
    "            \n",
    "            # stopping conditions\n",
    "            gradientNorm = np.linalg.norm(gradient)\n",
    "            \n",
    "            if gradientNorm < self.tolerance:\n",
    "                print('Gradient descent took', epoch, 'iterations to converge')\n",
    "                print('The norm of the gradient is', gradientNorm)\n",
    "                \n",
    "                # return the approximate critical value w\n",
    "                return self.w\n",
    "            \n",
    "            elif epoch == epochs - 1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                \n",
    "                # return the weights (sometimes it is still pretty good)\n",
    "                return self.w\n",
    "            \n",
    "            self.w -= self.alpha * gradient\n",
    "                            \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([n, 1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([n, 1]), X))\n",
    "        \n",
    "        # compute predictions  \n",
    "        return np.atleast_2d(expit(X @ self.w)).T\n",
    "    \n",
    "    def sigmoid(self, z): return np.expit(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                            \n",
       "1          20000    2          2         1   24      2      2     -1     -1   \n",
       "2         120000    2          2         2   26     -1      2      0      0   \n",
       "3          90000    2          2         2   34      0      0      0      0   \n",
       "4          50000    2          2         1   37      0      0      0      0   \n",
       "5          50000    1          2         1   57     -1      0     -1      0   \n",
       "...          ...  ...        ...       ...  ...    ...    ...    ...    ...   \n",
       "29996     220000    1          3         1   39      0      0      0      0   \n",
       "29997     150000    1          3         2   43     -1     -1     -1     -1   \n",
       "29998      30000    1          2         2   37      4      3      2     -1   \n",
       "29999      80000    1          3         1   41      1     -1      0      0   \n",
       "30000      50000    1          2         1   46      0      0      0      0   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "ID            ...                                                        \n",
       "1         -2  ...          0          0          0         0       689   \n",
       "2          0  ...       3272       3455       3261         0      1000   \n",
       "3          0  ...      14331      14948      15549      1518      1500   \n",
       "4          0  ...      28314      28959      29547      2000      2019   \n",
       "5          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29996      0  ...      88004      31237      15980      8500     20000   \n",
       "29997      0  ...       8979       5190          0      1837      3526   \n",
       "29998      0  ...      20878      20582      19357         0         0   \n",
       "29999      0  ...      52774      11855      48944     85900      3409   \n",
       "30000      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                                         \n",
       "1             0         0         0         0                           1  \n",
       "2          1000      1000         0      2000                           1  \n",
       "3          1000      1000      1000      5000                           0  \n",
       "4          1200      1100      1069      1000                           0  \n",
       "5         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29996      5003      3047      5000      1000                           0  \n",
       "29997      8998       129         0         0                           0  \n",
       "29998     22000      4200      2000      3100                           1  \n",
       "29999      1178      1926     52964      1804                           1  \n",
       "30000      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv('../data/credit_default.csv', sep = ',', header = 1, index_col = 'ID')\n",
    "\n",
    "# display the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks okay, but let's:\n",
    "\n",
    "1. Extract the labels from the right column, convert to a NumPy array, and drop the column\n",
    "2. Convert `SEX` and `MARRIAGE` to binary (0-1)\n",
    "3. Convert the categorical input `EDUCATION` into a one-hot representation\n",
    "4. Extract the data from all remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of trainX is (18000, 29)\n",
      "The dimensions of devX is (6000, 29)\n",
      "The dimensions of testX is (6000, 29)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/credit_default.csv', sep = ',', header = 1, index_col = 'ID')\n",
    "\n",
    "# extract the labels\n",
    "Y = df['default payment next month'].to_numpy()\n",
    "\n",
    "# drop the labels\n",
    "df = df.drop(columns = ['default payment next month'])\n",
    "\n",
    "# convert SEX and MARRIAGE into binary\n",
    "df['SEX'] -= 1\n",
    "df['MARRIAGE'] -= 1\n",
    "\n",
    "# convert EDUCATION to one-hot\n",
    "df = pd.get_dummies(df, columns = ['EDUCATION'])\n",
    "\n",
    "# extract the input data\n",
    "X = df.to_numpy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.4)\n",
    "\n",
    "# split the test set into dev and test sets\n",
    "devX, testX, devY, testY = train_test_split(testX, testY, test_size = 0.5)\n",
    "\n",
    "# check the dimensions\n",
    "print('The dimensions of trainX is', trainX.shape)\n",
    "print('The dimensions of devX is', devX.shape)\n",
    "print('The dimensions of testX is', devX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can build the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 5 iterations to converge\n",
      "The norm of the gradient is 8.339727627545374e-05\n",
      "\n",
      "Train Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.96      0.86     13974\n",
      "           1       0.15      0.02      0.04      4026\n",
      "\n",
      "    accuracy                           0.75     18000\n",
      "   macro avg       0.46      0.49      0.45     18000\n",
      "weighted avg       0.63      0.75      0.67     18000\n",
      "\n",
      "\n",
      "Dev Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86      4717\n",
      "           1       0.09      0.01      0.03      1283\n",
      "\n",
      "    accuracy                           0.76      6000\n",
      "   macro avg       0.44      0.49      0.44      6000\n",
      "weighted avg       0.63      0.76      0.68      6000\n",
      "\n",
      "\n",
      "Dev Confusion Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20189dc5108>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# build the linear regression model\n",
    "model = LeastSquaresClassifier(alpha = 0.00001)\n",
    "\n",
    "# fit the linear regression model to the training data\n",
    "model.fit(trainX, trainY, epochs = 1000)\n",
    "\n",
    "# predict the training Y's of the dev set\n",
    "predictedY = model.predict(trainX).round()\n",
    "\n",
    "print('\\nTrain Classification Report:\\n\\n', classification_report(trainY, predictedY))\n",
    "\n",
    "# predict the dev Y's\n",
    "predictedY = model.predict(devX).round()\n",
    "print('\\nDev Classification Report:\\n\\n', classification_report(devY, predictedY))\n",
    "print('\\nDev Confusion Matrix:\\n')\n",
    "\n",
    "sns.heatmap(confusion_matrix(devY, predictedY), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(run some hyperparameter experiments to improve performance hopefully)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
